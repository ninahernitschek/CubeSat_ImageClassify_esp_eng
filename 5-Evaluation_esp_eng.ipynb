{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac66dbc-7ba9-4115-98f7-f2470fb4b844",
   "metadata": {},
   "source": [
    "# Notebook 5: Evaluation / Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94be4a-3bc6-4ee8-aee8-76ff5de0c030",
   "metadata": {},
   "source": [
    "In this hackathon, you will use this notebook to evaluate the performance of your machine learning pipeline. The notebook is designed to assess the entire pipeline, from data preprocessing to making predictions, ensuring that your solution is practical, efficient, and accurate.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "En este hackathon, utilizarás este Jupyter notebook para evaluar el rendimiento de tu pipeline de aprendizaje automático. El cuaderno está diseñado para evaluar toda la canalización, desde el preprocesamiento de datos hasta la realización de predicciones, garantizando que tu solución sea práctica, eficiente y precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85f400",
   "metadata": {},
   "source": [
    "**Important Note:**\n",
    "- The evaluation runs on a `single` CPU core to ensure fair resource usage comparisons.\n",
    "- It assesses the pipeline’s performance as a whole, not just the final model’s predictions.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "**Importante:**\n",
    "- La evaluación se ejecuta en un único núcleo de CPU para garantizar un uso equitativo de los recursos.\n",
    "- Evalúa el rendimiento del pipeline en su conjunto, no sólo las predicciones del modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec781abf-7ce3-4740-8a88-35e9aae2e2a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c08a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. What the Evaluation Measures / Qué mide la evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8ea02",
   "metadata": {},
   "source": [
    "The notebook evaluates your pipeline on several metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31acc6d",
   "metadata": {},
   "source": [
    "#### **1.\tAccuracy and Model Performance:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486a20",
   "metadata": {},
   "source": [
    "- **Accuracy**: How many predictions were correct.\n",
    "- **F1 Score**: A balanced measure of precision and recall, especially for imbalanced datasets.\n",
    "- **Confusion Matrix**: A detailed breakdown of your model’s predictions across the following classes:\n",
    "    - `Priority`: The most important class, representing clear and usable images. Misclassifying these has the greatest negative impact.\n",
    "\t- `Noisy` & `Blurry`: Impure images that could potentially be recovered through preprocessing. Errors here are less critical but still significant.\n",
    "\t- `Corrupt` & `Missing_Data`: The least important classes, representing images with severe issues or no usable data. Misclassifications here have the smallest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc913b73",
   "metadata": {},
   "source": [
    "#### **2. Evaluation Time:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26620e45",
   "metadata": {},
   "source": [
    "- **What we measure**: How long your pipeline takes to preprocess the data and make predictions.\n",
    "- **Single CPU Core**: The evaluation runs on a `single` CPU core, not just for fairness, but to simulate a resource-constrained environment where computational resources are limited.\n",
    "- **Why it matters**: Faster pipelines are more practical and scalable, especially in environments with strict performance or resource limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021850c8",
   "metadata": {},
   "source": [
    "#### **3. Memory Usage:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c8f511",
   "metadata": {},
   "source": [
    "- **Peak Memory Usage:** Maximum memory used during pipeline execution.\n",
    "- **Average CPU Usage:** Percentage of a single core used throughout the process.\n",
    "- **Why it matters:** CubeSat will use a processor with limited memory, and the memory utilization must fit within available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc8794",
   "metadata": {},
   "source": [
    "#### **4. Algorithm Code Size:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cd4fc",
   "metadata": {},
   "source": [
    "- The total size of your pipeline, including the serialized model and preprocessing function, measured in megabytes (MB).\n",
    "- Algorithm code must fit within a limited available memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278dbf7",
   "metadata": {},
   "source": [
    " <hr style=\"border:2px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178208e",
   "metadata": {},
   "source": [
    "#### **1.\tPrecisión y rendimiento del modelo:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89061d7a",
   "metadata": {},
   "source": [
    "- **Precisión**: Cuántas predicciones fueron correctas.\n",
    "- **Puntuación F1**: Una medida equilibrada de precisión y recuperación, especialmente para conjuntos de datos desequilibrados.\n",
    "- **Matriz de confusión**: Un desglose detallado de las predicciones de su modelo en las siguientes clases:\n",
    "    - `Priority`: La clase más importante, que representa imágenes claras y utilizables. Las clasificaciones erróneas tienen el mayor impacto negativo.\n",
    "\t- `Noisy` & `Blurry`: Imágenes impuras que podrían recuperarse mediante preprocesamiento. En este caso, los errores son menos graves, pero no por ello menos importantes.\n",
    "\t- `Corrupt` & `Missing_Data`: Las clases menos importantes, que representan imágenes con problemas graves o sin datos utilizables. Los errores de clasificación tienen aquí el menor impacto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19f89d",
   "metadata": {},
   "source": [
    "#### **2. Tiempo de evaluación:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee9ea8",
   "metadata": {},
   "source": [
    "- **Qué medimos**: Cuánto tarda tu pipeline en preprocesar los datos y hacer predicciones.\n",
    "- **Un solo núcleo de CPU**: La evaluación se ejecuta en un **único** núcleo de CPU, no sólo por motivos de equidad, sino también para simular un entorno con recursos limitados.\n",
    "- **Por qué es importante**: Los canales más rápidos son más prácticos y escalables, especialmente en entornos con limitaciones estrictas de rendimiento o recursos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3bca0c",
   "metadata": {},
   "source": [
    "#### **3. Utilización de la memoria:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125ad83",
   "metadata": {},
   "source": [
    "- **Uso máximo de memoria:** Memoria máxima utilizada durante la ejecución del pipeline.\n",
    "- **Uso medio de CPU:** Porcentaje de un solo núcleo utilizado durante todo el proceso.\n",
    "- **Por qué es importante:** CubeSat utilizará un procesador con memoria limitada, y la utilización de la memoria debe ajustarse a la memoria disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dfc11",
   "metadata": {},
   "source": [
    "#### **4. Algoritmo Tamaño del código:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532ba99",
   "metadata": {},
   "source": [
    "- El tamaño total de su pipeline, incluido el modelo serializado y la función de preprocesamiento, medido en megabytes (MB).\n",
    "- El código del algoritmo debe caber en una memoria disponible limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faea8bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368b8d6",
   "metadata": {},
   "source": [
    "#### Import test data / Importar datos de prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# first let us load the testing data\n",
    "test_images = np.load('data/test_images.npy')      # Load image test data\n",
    "test_labels = np.load('data/test_labels.npy')      # Load label test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6882674",
   "metadata": {},
   "source": [
    "**Note:** You can also use this notebook to evaluate trained models on the validation set.\n",
    "    \n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "**Nota:** También puede utilizar este Jupyter notebook para evaluar los modelos entrenados en el conjunto de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7420b",
   "metadata": {},
   "source": [
    "#### Import Evaluation function / Importar la función de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e480ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.evaluate import evaluate_pipeline \n",
    "# A built-in function to evaluate a given ML pipeline by preprocessing, predicting, and calculating performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94354517",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "  \n",
    "- **model**: The trained machine learning model to evaluate.\n",
    "- **X_test_raw**: Raw test data that needs to be preprocessed before evaluation.\n",
    "- **y_test**: True labels corresponding to the test data for performance comparison.\n",
    "- **preprocessing_fn**: A function used to preprocess the raw test data.\n",
    "    \n",
    "**Outputs:**\n",
    "\n",
    "- **metrics**: A dictionary containing various evaluation metrics like accuracy, F1 score, evaluation time, memory usage, CPU usage, and algorithm code size.\n",
    "- **confusion matrix**\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "**Entradas:**\n",
    "  \n",
    "- `model`: El modelo de aprendizaje automático entrenado para evaluar.\n",
    "- `X_test_raw`: Datos de prueba sin procesar que necesitan ser preprocesados antes de la evaluación.\n",
    "- `y_test`: Etiquetas verdaderas correspondientes a los datos de prueba para comparar el rendimiento.\n",
    "- `preprocessing_fn`: Función utilizada para preprocesar los datos de prueba sin procesar.\n",
    "    \n",
    "**Resultados:**\n",
    "\n",
    "- `metrics`: Un diccionario que contiene varias métricas de evaluación como precisión, puntuación F1, tiempo de evaluación, uso de memoria, uso de CPU y tamaño del código del algoritmo.\n",
    "- matriz de confusión\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc978e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47c46b",
   "metadata": {},
   "source": [
    "### 2. ML: Evaluation (3rd notebook) / Evaluación del aprendizaje automático (Jupyter notebook 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2f5bb",
   "metadata": {},
   "source": [
    "#### Preprocessing (Testing data) / Preprocesamiento (datos de prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabac7c",
   "metadata": {},
   "source": [
    "Applying consistent preprocessing to both training and testing data ensures accurate predictions, prevents mismatches, and improves model evaluation. Combining all steps into one function enhances consistency, tracks execution time, and optimizes resource usage.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "La aplicación de un preprocesamiento coherente tanto a los datos de entrenamiento como a los de prueba garantiza predicciones precisas, evita desajustes y mejora la evaluación de los modelos. Combinar todos los pasos en una sola función mejora la coherencia, controla el tiempo de ejecución y optimiza el uso de recursos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_ML(X): # We apply the same pre-processing steps implemented in Notebook 3.\n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    # Normalize the data to [0, 1]\n",
    "    X_pre = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    X_pre = np.array([rgb2gray(image) for image in X_pre])\n",
    "    \n",
    "    # Resize images to 64x64 pixels\n",
    "    X_pre = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X_pre])\n",
    "    \n",
    "    # Flatten the images\n",
    "    num_samples = X_pre.shape[0]\n",
    "    X_pre = X_pre.reshape(num_samples, -1)\n",
    "    \n",
    "    return X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import ML model / Importar modelo de aprendizaje automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d626df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the ml model from the 3rd notebook\n",
    "with open('models/sgd_model.pkl', 'rb') as file:\n",
    "    sgd_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f8e8c",
   "metadata": {},
   "source": [
    "#### ML Evaluation / Evaluación de modelo de aprendizaje automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have:\n",
    "# - A trained model named like 'lr_model'\n",
    "# - Raw test data 'X_test_raw'\n",
    "# - True labels 'y_test'\n",
    "# - All pre-processing methods gathered in one function\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(sgd_model, test_images, test_labels, preprocessing_fn_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7ed17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e9dcf",
   "metadata": {},
   "source": [
    "### 3. CubeSatNet_CNN Evaluation / Evaluación de CubeSatNet_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e2086",
   "metadata": {},
   "source": [
    "#### preprocessing / preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7674a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_CNN(X):  # we did not use any preprocessing in notebook 4\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62212d91",
   "metadata": {},
   "source": [
    "#### Import CNN model / Importar modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "# Load the CNN model from the 4th notebook\n",
    "with open('models/cnn_model.pkl', 'rb') as file:\n",
    "    cnn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0eeb16",
   "metadata": {},
   "source": [
    "#### Evaluate the CNN pipeline /  Evaluar el CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823da435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(cnn_model, test_images, test_labels, preprocessing_fn_CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067920c",
   "metadata": {},
   "source": [
    "##### **⚠️ Freeing up Space** / **⚠️ Liberar espacio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Remove the data from memory\n",
    "del sgd_model, cnn_model, test_images, test_labels\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the input/output cache\n",
    "print(\"Data and models removed from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2533b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4ace3",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "1. You can also use the `evaluate_pipeline` function to evaluate your validation set data.\n",
    "2. Please do not modify the `evaluate_pipeline` function.\n",
    "\n",
    "Best of luck with the hackathon challenge! 🎉 We look forward to seeing your final results and pipelines! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9eb984",
   "metadata": {},
   "source": [
    "\n",
    " <hr style=\"border:2px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd07b98",
   "metadata": {},
   "source": [
    "**Nota:**\n",
    "\n",
    "1. También puede utilizar la función `evaluate_pipeline` para evaluar los datos de su conjunto de validación.\n",
    "2. Por favor, no modifique la función `evaluate_pipeline`.\n",
    "\n",
    "¡Mucha suerte con el reto del hackathon! 🎉 ¡Estamos deseando ver tus resultados finales y pipelines! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafabda-93ba-4a02-b6ba-822b518ac6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
