{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac66dbc-7ba9-4115-98f7-f2470fb4b844",
   "metadata": {},
   "source": [
    "# Notebook 5: Evaluation / Evaluaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94be4a-3bc6-4ee8-aee8-76ff5de0c030",
   "metadata": {},
   "source": [
    "In this hackathon, you will use this notebook to evaluate the performance of your machine learning pipeline. The notebook is designed to assess the entire pipeline, from data preprocessing to making predictions, ensuring that your solution is practical, efficient, and accurate.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "En este hackathon, utilizar谩s este Jupyter notebook para evaluar el rendimiento de tu pipeline de aprendizaje autom谩tico. El cuaderno est谩 dise帽ado para evaluar toda la canalizaci贸n, desde el preprocesamiento de datos hasta la realizaci贸n de predicciones, garantizando que tu soluci贸n sea pr谩ctica, eficiente y precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85f400",
   "metadata": {},
   "source": [
    "**Important Note:**\n",
    "- The evaluation runs on a `single` CPU core to ensure fair resource usage comparisons.\n",
    "- It assesses the pipelines performance as a whole, not just the final models predictions.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "**Importante:**\n",
    "- La evaluaci贸n se ejecuta en un 煤nico n煤cleo de CPU para garantizar un uso equitativo de los recursos.\n",
    "- Eval煤a el rendimiento del pipeline en su conjunto, no s贸lo las predicciones del modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec781abf-7ce3-4740-8a88-35e9aae2e2a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c08a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. What the Evaluation Measures / Qu茅 mide la evaluaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8ea02",
   "metadata": {},
   "source": [
    "The notebook evaluates your pipeline on several metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31acc6d",
   "metadata": {},
   "source": [
    "#### **1.\tAccuracy and Model Performance:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29486a20",
   "metadata": {},
   "source": [
    "- **Accuracy**: How many predictions were correct.\n",
    "- **F1 Score**: A balanced measure of precision and recall, especially for imbalanced datasets.\n",
    "- **Confusion Matrix**: A detailed breakdown of your models predictions across the following classes:\n",
    "    - `Priority`: The most important class, representing clear and usable images. Misclassifying these has the greatest negative impact.\n",
    "\t- `Noisy` & `Blurry`: Impure images that could potentially be recovered through preprocessing. Errors here are less critical but still significant.\n",
    "\t- `Corrupt` & `Missing_Data`: The least important classes, representing images with severe issues or no usable data. Misclassifications here have the smallest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc913b73",
   "metadata": {},
   "source": [
    "#### **2. Evaluation Time:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26620e45",
   "metadata": {},
   "source": [
    "- **What we measure**: How long your pipeline takes to preprocess the data and make predictions.\n",
    "- **Single CPU Core**: The evaluation runs on a `single` CPU core, not just for fairness, but to simulate a resource-constrained environment where computational resources are limited.\n",
    "- **Why it matters**: Faster pipelines are more practical and scalable, especially in environments with strict performance or resource limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021850c8",
   "metadata": {},
   "source": [
    "#### **3. Memory Usage:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c8f511",
   "metadata": {},
   "source": [
    "- **Peak Memory Usage:** Maximum memory used during pipeline execution.\n",
    "- **Average CPU Usage:** Percentage of a single core used throughout the process.\n",
    "- **Why it matters:** CubeSat will use a processor with limited memory, and the memory utilization must fit within available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc8794",
   "metadata": {},
   "source": [
    "#### **4. Algorithm Code Size:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cd4fc",
   "metadata": {},
   "source": [
    "- The total size of your pipeline, including the serialized model and preprocessing function, measured in megabytes (MB).\n",
    "- Algorithm code must fit within a limited available memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278dbf7",
   "metadata": {},
   "source": [
    " <hr style=\"border:2px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178208e",
   "metadata": {},
   "source": [
    "#### **1.\tPrecisi贸n y rendimiento del modelo:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89061d7a",
   "metadata": {},
   "source": [
    "- **Precisi贸n**: Cu谩ntas predicciones fueron correctas.\n",
    "- **Puntuaci贸n F1**: Una medida equilibrada de precisi贸n y recuperaci贸n, especialmente para conjuntos de datos desequilibrados.\n",
    "- **Matriz de confusi贸n**: Un desglose detallado de las predicciones de su modelo en las siguientes clases:\n",
    "    - `Priority`: La clase m谩s importante, que representa im谩genes claras y utilizables. Las clasificaciones err贸neas tienen el mayor impacto negativo.\n",
    "\t- `Noisy` & `Blurry`: Im谩genes impuras que podr铆an recuperarse mediante preprocesamiento. En este caso, los errores son menos graves, pero no por ello menos importantes.\n",
    "\t- `Corrupt` & `Missing_Data`: Las clases menos importantes, que representan im谩genes con problemas graves o sin datos utilizables. Los errores de clasificaci贸n tienen aqu铆 el menor impacto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19f89d",
   "metadata": {},
   "source": [
    "#### **2. Tiempo de evaluaci贸n:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee9ea8",
   "metadata": {},
   "source": [
    "- **Qu茅 medimos**: Cu谩nto tarda tu pipeline en preprocesar los datos y hacer predicciones.\n",
    "- **Un solo n煤cleo de CPU**: La evaluaci贸n se ejecuta en un **煤nico** n煤cleo de CPU, no s贸lo por motivos de equidad, sino tambi茅n para simular un entorno con recursos limitados.\n",
    "- **Por qu茅 es importante**: Los canales m谩s r谩pidos son m谩s pr谩cticos y escalables, especialmente en entornos con limitaciones estrictas de rendimiento o recursos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3bca0c",
   "metadata": {},
   "source": [
    "#### **3. Utilizaci贸n de la memoria:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125ad83",
   "metadata": {},
   "source": [
    "- **Uso m谩ximo de memoria:** Memoria m谩xima utilizada durante la ejecuci贸n del pipeline.\n",
    "- **Uso medio de CPU:** Porcentaje de un solo n煤cleo utilizado durante todo el proceso.\n",
    "- **Por qu茅 es importante:** CubeSat utilizar谩 un procesador con memoria limitada, y la utilizaci贸n de la memoria debe ajustarse a la memoria disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dfc11",
   "metadata": {},
   "source": [
    "#### **4. Algoritmo Tama帽o del c贸digo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532ba99",
   "metadata": {},
   "source": [
    "- El tama帽o total de su pipeline, incluido el modelo serializado y la funci贸n de preprocesamiento, medido en megabytes (MB).\n",
    "- El c贸digo del algoritmo debe caber en una memoria disponible limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faea8bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368b8d6",
   "metadata": {},
   "source": [
    "#### Import test data / Importar datos de prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# first let us load the testing data\n",
    "test_images = np.load('data/test_images.npy')      # Load image test data\n",
    "test_labels = np.load('data/test_labels.npy')      # Load label test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6882674",
   "metadata": {},
   "source": [
    "**Note:** You can also use this notebook to evaluate trained models on the validation set.\n",
    "    \n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "**Nota:** Tambi茅n puede utilizar este Jupyter notebook para evaluar los modelos entrenados en el conjunto de validaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7420b",
   "metadata": {},
   "source": [
    "#### Import Evaluation function / Importar la funci贸n de evaluaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e480ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.evaluate import evaluate_pipeline \n",
    "# A built-in function to evaluate a given ML pipeline by preprocessing, predicting, and calculating performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94354517",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "  \n",
    "- **model**: The trained machine learning model to evaluate.\n",
    "- **X_test_raw**: Raw test data that needs to be preprocessed before evaluation.\n",
    "- **y_test**: True labels corresponding to the test data for performance comparison.\n",
    "- **preprocessing_fn**: A function used to preprocess the raw test data.\n",
    "    \n",
    "**Outputs:**\n",
    "\n",
    "- **metrics**: A dictionary containing various evaluation metrics like accuracy, F1 score, evaluation time, memory usage, CPU usage, and algorithm code size.\n",
    "- **confusion matrix**\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "**Entradas:**\n",
    "  \n",
    "- `model`: El modelo de aprendizaje autom谩tico entrenado para evaluar.\n",
    "- `X_test_raw`: Datos de prueba sin procesar que necesitan ser preprocesados antes de la evaluaci贸n.\n",
    "- `y_test`: Etiquetas verdaderas correspondientes a los datos de prueba para comparar el rendimiento.\n",
    "- `preprocessing_fn`: Funci贸n utilizada para preprocesar los datos de prueba sin procesar.\n",
    "    \n",
    "**Resultados:**\n",
    "\n",
    "- `metrics`: Un diccionario que contiene varias m茅tricas de evaluaci贸n como precisi贸n, puntuaci贸n F1, tiempo de evaluaci贸n, uso de memoria, uso de CPU y tama帽o del c贸digo del algoritmo.\n",
    "- matriz de confusi贸n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc978e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47c46b",
   "metadata": {},
   "source": [
    "### 2. ML: Evaluation (3rd notebook) / Evaluaci贸n del aprendizaje autom谩tico (Jupyter notebook 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2f5bb",
   "metadata": {},
   "source": [
    "#### Preprocessing (Testing data) / Preprocesamiento (datos de prueba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabac7c",
   "metadata": {},
   "source": [
    "Applying consistent preprocessing to both training and testing data ensures accurate predictions, prevents mismatches, and improves model evaluation. Combining all steps into one function enhances consistency, tracks execution time, and optimizes resource usage.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "La aplicaci贸n de un preprocesamiento coherente tanto a los datos de entrenamiento como a los de prueba garantiza predicciones precisas, evita desajustes y mejora la evaluaci贸n de los modelos. Combinar todos los pasos en una sola funci贸n mejora la coherencia, controla el tiempo de ejecuci贸n y optimiza el uso de recursos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_ML(X): # We apply the same pre-processing steps implemented in Notebook 3.\n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    # Normalize the data to [0, 1]\n",
    "    X_pre = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    X_pre = np.array([rgb2gray(image) for image in X_pre])\n",
    "    \n",
    "    # Resize images to 64x64 pixels\n",
    "    X_pre = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X_pre])\n",
    "    \n",
    "    # Flatten the images\n",
    "    num_samples = X_pre.shape[0]\n",
    "    X_pre = X_pre.reshape(num_samples, -1)\n",
    "    \n",
    "    return X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import ML model / Importar modelo de aprendizaje autom谩tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d626df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the ml model from the 3rd notebook\n",
    "with open('models/sgd_model.pkl', 'rb') as file:\n",
    "    sgd_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f8e8c",
   "metadata": {},
   "source": [
    "#### ML Evaluation / Evaluaci贸n de modelo de aprendizaje autom谩tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have:\n",
    "# - A trained model named like 'lr_model'\n",
    "# - Raw test data 'X_test_raw'\n",
    "# - True labels 'y_test'\n",
    "# - All pre-processing methods gathered in one function\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(sgd_model, test_images, test_labels, preprocessing_fn_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7ed17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e9dcf",
   "metadata": {},
   "source": [
    "### 3. CubeSatNet_CNN Evaluation / Evaluaci贸n de CubeSatNet_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e2086",
   "metadata": {},
   "source": [
    "#### preprocessing / preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7674a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_CNN(X):  # we did not use any preprocessing in notebook 4\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62212d91",
   "metadata": {},
   "source": [
    "#### Import CNN model / Importar modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "# Load the CNN model from the 4th notebook\n",
    "with open('models/cnn_model.pkl', 'rb') as file:\n",
    "    cnn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0eeb16",
   "metadata": {},
   "source": [
    "#### Evaluate the CNN pipeline /  Evaluar el CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823da435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(cnn_model, test_images, test_labels, preprocessing_fn_CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067920c",
   "metadata": {},
   "source": [
    "##### **锔 Freeing up Space** / **锔 Liberar espacio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Remove the data from memory\n",
    "del sgd_model, cnn_model, test_images, test_labels\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the input/output cache\n",
    "print(\"Data and models removed from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2533b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4ace3",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "1. You can also use the `evaluate_pipeline` function to evaluate your validation set data.\n",
    "2. Please do not modify the `evaluate_pipeline` function.\n",
    "\n",
    "Best of luck with the hackathon challenge!  We look forward to seeing your final results and pipelines! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9eb984",
   "metadata": {},
   "source": [
    "\n",
    " <hr style=\"border:2px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd07b98",
   "metadata": {},
   "source": [
    "**Nota:**\n",
    "\n",
    "1. Tambi茅n puede utilizar la funci贸n `evaluate_pipeline` para evaluar los datos de su conjunto de validaci贸n.\n",
    "2. Por favor, no modifique la funci贸n `evaluate_pipeline`.\n",
    "\n",
    "隆Mucha suerte con el reto del hackathon!  隆Estamos deseando ver tus resultados finales y pipelines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafabda-93ba-4a02-b6ba-822b518ac6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
