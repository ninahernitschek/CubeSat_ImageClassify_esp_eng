{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df38459-6403-4c59-8bf9-d4f00192f492",
   "metadata": {},
   "source": [
    "# Notebook 3: Classification Using Machine Learning / Clasificación mediante aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324361f-daa8-4d9e-a5b4-3ac5465c9a6b",
   "metadata": {},
   "source": [
    "In this notebook, we will use a classical machine learning method to classify the astronomical data saved in the previous notebook.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    "En este Jupyter notebook, utilizaremos un método clásico de aprendizaje automático para clasificar los datos astronómicos guardados en el Jupyter notebook anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4c998-a020-48c1-b7b0-1b3a755e733e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c7fbc-fa08-4312-ae32-88e30eb1d673",
   "metadata": {},
   "source": [
    "### Reading the data /  Lectura de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811c1b4-4512-4139-99a5-469776428dcc",
   "metadata": {},
   "source": [
    "First, we'll load the saved image and label data from the NumPy files.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    " \n",
    "En primer lugar, cargaremos la imagen guardada y los datos de la etiqueta de los archivos NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f6b6f-23c3-4017-b263-4d6204a44996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importing NumPy for numerical operations and array handling\n",
    "\n",
    "# Load the training images and labels back from the saved NumPy files\n",
    "train_images = np.load('data/train_images.npy')  # Load image training data\n",
    "train_labels = np.load('data/train_labels.npy')  # Load label training data\n",
    "val_images = np.load('data/val_images.npy')  # Load image training data\n",
    "val_labels = np.load('data/val_labels.npy')  # Load label training data\n",
    "\n",
    "print(\"Training Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e2a76-0d8b-4f5f-9c6d-a1d15474a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd935ff-1587-4a25-91ca-80dc157bf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204e579-2d19-4b8f-9a9f-f4af212b3417",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a94cb-00bd-4e79-ad42-ebc9652fbf18",
   "metadata": {},
   "source": [
    "## Pre-processing / Pre-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b7ae3-5950-44f2-80c2-749076b1ab0a",
   "metadata": {},
   "source": [
    "We will pre-process the training images to simplify the data by:\n",
    "\n",
    "1. `Grayscaling`\n",
    "2. `Normalizing` to range [0, 1]\n",
    "3. `Downscaling` to a target size of [64x64].\n",
    "4. `Flattening` the images to be ready for machine learning models.\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    "Preprocesaremos las imágenes de entrenamiento para simplificar los datos mediante:\n",
    "\n",
    "1. `Grayscaling`\n",
    "2. `Normalizing`  al rango [0, 1].\n",
    "3. `Downscaling` a un tamaño objetivo de [64x64].\n",
    "4. `Flattening` las imágenes para prepararlas para los modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_ML(X): \n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    # Normalize the data to [0, 1]\n",
    "    X_pre = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    X_pre = np.array([rgb2gray(image) for image in X_pre])\n",
    "    \n",
    "    # Resize images to 64x64 pixels\n",
    "    X_pre = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X_pre])\n",
    "    \n",
    "    # Flatten the images\n",
    "    num_samples = X_pre.shape[0]\n",
    "    X_pre = X_pre.reshape(num_samples, -1)\n",
    "    \n",
    "    return X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d847f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_pre = preprocessing_fn_ML(train_images)\n",
    "val_images_pre = preprocessing_fn_ML(val_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8d70a",
   "metadata": {},
   "source": [
    "##### **⚠️ Freeing up Space** / **⚠️ Liberar espacio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "# Since we will no longer need the original data, we can remove it from memory\n",
    "del train_images, val_images\n",
    "\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"Original images removed from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572e2ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383573e3",
   "metadata": {},
   "source": [
    "### Visualising the data after pre-processing / Visualizando los datos después del pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the class names\n",
    "class_names = [\"Blurry\", \"Corrupt\", \"Missing_Data\", \"Noisy\", \"Priority\"]\n",
    "\n",
    "# Get the unique labels in the training set\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "# Display the first 5 images for each class after preprocessing\n",
    "for label in unique_labels:\n",
    "    # Find the indices of images belonging to the current class\n",
    "    class_indices = np.where(train_labels == label)[0]\n",
    "    \n",
    "    # Select the first 5 images of this class\n",
    "    num_images_to_display = min(5, len(class_indices))\n",
    "    selected_indices = class_indices[:num_images_to_display]\n",
    "    selected_images = train_images_pre[selected_indices]  # Already preprocessed images\n",
    "\n",
    "    # Plot the selected images\n",
    "    fig, axes = plt.subplots(1, num_images_to_display, figsize=(20, 4))\n",
    "    fig.suptitle(f'Class: {class_names[label]}', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(selected_images[i].reshape(64, 64), cmap='gray')  # Reshape and use grayscale colormap\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fe32e",
   "metadata": {},
   "source": [
    "\n",
    "After reviewing the pre-processed images, it appears that reducing the size from 512x512 to 64x64 may not have been the best choice. Visually, it has become more challenging to distinguish between the **Priority**, **Noisy**, and **Blurry** categories. Now, let’s apply a machine learning method to classify these images.\n",
    "\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    "Tras revisar las imágenes preprocesadas, parece que reducir el tamaño de 512x512 a 64x64 puede no haber sido la mejor elección. Visualmente, se ha vuelto más difícil distinguir entre las categorías **Priority**, **Noisy** y **Blurry**. Ahora, apliquemos un método de aprendizaje automático para clasificar estas imágenes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3950ac-b778-440e-9d1c-b2acdcb42f95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920f428-c5a6-4f7b-a07a-52392896719e",
   "metadata": {},
   "source": [
    "## ML Classification / Clasificación por aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77290125-274d-4084-a0e9-1ebca479a52e",
   "metadata": {},
   "source": [
    "### Train the Stochastic Gradient Descent (SGD) Model / Entrenar el modelo de descenso gradiente estocástico (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1105699-f55f-4208-bc3f-1797f82a3e9d",
   "metadata": {},
   "source": [
    "The Stochastic Gradient Descent (SGD) model uses the SGD optimization method to train machine learning models. The SGDClassifier is effective for efficiently training linear models, particularly on large datasets. However, it may not perform well on complex, non-linear data such as images.\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    " \n",
    "El modelo de Descenso Gradiente Estocástico (SGD) hace referencia a los algoritmos que utilizan el método de optimización de descenso gradiente estocástico para entrenar modelos de aprendizaje automático. El SGDClassifier es una potente herramienta para entrenar modelos lineales de forma eficiente, especialmente cuando se trata de grandes conjuntos de datos. Su velocidad procede de la actualización incremental de los parámetros del modelo utilizando muestras individuales o pequeños lotes, lo que reduce significativamente la sobrecarga computacional por iteración en comparación con los métodos tradicionales de descenso de gradiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5efcd-59d1-411f-b666-52bac92e0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import pickle\n",
    "\n",
    "# Create the stochastic gradient descent model\n",
    "\n",
    "sgd_model = SGDClassifier( loss='log_loss', max_iter=10000, n_jobs=4, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "sgd_model.fit(train_images_pre, train_labels)\n",
    "\n",
    "# Save the model to a file\n",
    "with open('sgd_model.pkl', 'wb') as file:\n",
    "    pickle.dump(sgd_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8545012-9a47-4340-a4ca-6fc3bf204502",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41027dd0-ec8f-4a99-97ab-c4420cb2d6e0",
   "metadata": {},
   "source": [
    "### ML: Validation set results / Resultados del conjunto de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39db5a2",
   "metadata": {},
   "source": [
    "**Validation sets** are commonly used to evaluate the accuracy of machine learning models and guide improvements in their performance. Participants can modify the model **parameters** or adjust the **pre-processing** steps, then retrain the model. The results on the **validation set** help determine if these changes have improved the model’s performance. This iterative testing process should be performed exclusively on the **validation set**, not the **testing set**, to prevent overfitting and ensure the model generalizes well to unseen data.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "Los **conjuntos de validación** se utilizan habitualmente para evaluar la precisión de los modelos de aprendizaje automático y orientar las mejoras en su rendimiento. Los participantes pueden modificar los **parámetros** del modelo o ajustar los **pasos de preprocesamiento** y, a continuación, volver a entrenar el modelo. Los resultados en el **conjunto de validación** ayudan a determinar si estos cambios han mejorado el rendimiento del modelo. Este proceso de prueba iterativo debe realizarse exclusivamente en el **conjunto de validación**, no en el **conjunto de prueba**, para evitar el sobreajuste y garantizar que el modelo se generaliza bien a los datos no vistos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b23cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sgd_model.pkl', 'rb') as file:\n",
    "    sgd_loaded_model = pickle.load(file)\n",
    "    \n",
    "val_predictions = sgd_loaded_model.predict(val_images_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bacf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"Blurry\", \"Corrupt\", \"Missing_Data\", \"Noisy\", \"Priority\"]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_predictions)\n",
    "\n",
    "# Plot the confusion matrix with class names\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "# Customize and display the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # Set the figure size\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')  # Use a blue colormap\n",
    "plt.title(\"Confusion Matrix with Class Names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1bef4",
   "metadata": {},
   "source": [
    "The machine learning model is performing poorly. The confusion matrix reveals significant misclassification, particularly between the `Priority` class and the `Noisy` and `Blurry` classes. This observation aligns with our analysis when we visualized the pre-processed images.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "    \n",
    "El modelo de aprendizaje automático funciona mal. La matriz de confusión revela una clasificación errónea significativa, en particular entre la clase `Priority` y las clases `Noisy` y Blurry`. Esta observación coincide con nuestro análisis cuando visualizamos las imágenes preprocesadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b0674",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931d1dd",
   "metadata": {},
   "source": [
    "##### **⚠️ Freeing up Space** / **⚠️ Liberar espacio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Remove the data from memory\n",
    "del train_images_pre, train_labels, val_images_pre, val_labels\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the input/output cache\n",
    "print(\"Data removed from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Restart the kernel programmatically to free up space\n",
    "os.execv(sys.executable, ['python'] + sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
